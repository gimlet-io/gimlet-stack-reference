---
name: Gimlet Stack Reference
description: Providing a reference stack to demonstrate Gimlet Stack
categories:
  - name: "‚¨ÖÔ∏èIngress"
    id: ingress
  - name: "üìë Logging"
    id: logging
  - name: "üî¢ Metrics"
    id: metrics
  - name: "üîê Secrets"
    id: secrets
  - name: "üîç Tracing"
    id: tracing
  - name: "üìã Policy"
    id: policy
components:
  - name: Nginx
    variable: nginx
    category: ingress
    logo: https://raw.githubusercontent.com/gimlet-io/gimlet-stack-reference/main/assets/nginx.png
    description: 'An Nginx proxy server that routes traffic to your applications based on the host name or path.'
    onePager: |-
      ### What do you get with Nginx?

      An Nginx proxy server that routes traffic to your applications based on the host name or path.

      ### How to verify the deployment?

      ```
      $ kubectl get pods,services --namespace infrastructure
      NAME                                                   READY   STATUS    RESTARTS   AGE
      nginx-nginx-ingress-default-backend-6d96c457f6-hfkn8   1/1     Running   0          114s
      nginx-nginx-ingress-controller-6874d7c7f-l7dzc         1/1     Running   0          114s

      NAME                                          TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
      service/nginx-nginx-ingress-controller        LoadBalancer   10.43.199.76    1.2.3.4       80:30377/TCP,443:31126/TCP   114s
      service/nginx-nginx-ingress-default-backend   ClusterIP      10.43.176.181   <none>        80/TCP                       114s
      ```

      It started the ingress controller pod and the default-backend pod that is served on HTTP 404 requests.

      ### Pointing a DNS entry to the ingress IP address

      Locate the external IP of the `nginx-nginx-ingress-controller` service. On local clusters this will always be `Pending`, but on managed Kubernetes providers, the IP is set within a couple of minutes.

      You need to create now DNS entry for this IP. A wildcard DNS on `*.yourdomain.com` is preferred for this purpose.

      To validate the ingress, let's use xip.io, which is a dynamic DNS service: it provides wildcard DNS for any IP address. Say your LAN IP address is 10.0.0.1.
      Using xip.io, 10.0.0.1.xip.io resolves to 10.0.0.1

      Now try [https://test.<<yourIP>>.xip.io](https://test.<<yourIP>>.xip.io) which will return HTTP 404, served by the just deployed default backend. Indicating that the ingress controller works.

      Next steps:
      - add a wildcard DNS entry that points to the `nginx-nginx-ingress-controller` service's IP
      - add a [Kubernetes ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) object to one of your applications.
    schema: |-
      {
        "$schema": "http://json-schema.org/draft-07/schema",
        "$id": "#nginx",
        "type": "object",
        "properties": {
          "enabled": {
            "$id": "#/properties/enabled",
            "type": "boolean",
            "title": "Enabled"
          }
        },
        "dependencies": {
          "enabled": {
            "oneOf": [
              {
                "properties": {
                  "enabled": {
                    "const": false
                  }
                }
              },
              {
                "properties": {
                  "enabled": {
                    "const": true
                  },
                  "host": {
                    "$id": "#/properties/host",
                    "type": "string",
                    "title": "Host",
                    "description": "Your company domain you will expose your services on"
                  }
                },
                "required": [
                  "host"
                ]
              }
            ]
          }
        }
      }
    uiSchema: |-
      [
        {
          "schemaIDs": [
            "#nginx"
          ],
          "uiSchema": {},
          "metaData": {}
        }
      ]
  - name: CertManager
    variable: certManager
    category: ingress
    logo: https://raw.githubusercontent.com/gimlet-io/gimlet-stack-reference/main/assets/certManager.png
    description: ''
    onePager: |-
      ### What do you get with Cert Manager?

      Free SSL certificates for all your applications from Let's Encrypt

      ### How to verify the deployment?

      ```
      $ kubectl get pods,clusterissuer --namespace infrastructure
      NAME                                                   READY   STATUS    RESTARTS   AGE
      cert-manager-cainjector-5c88c48f9-pxc6p                1/1     Running   0          5m2s
      cert-manager-75d94494d6-7zzg4                          1/1     Running   0          5m2s
      cert-manager-webhook-864997b596-jfdpz                  1/1     Running   0          5m2s

      NAME                                        READY   AGE
      clusterissuer.cert-manager.io/letsencrypt   True    1m
      ```

      Once the `ClusterIssuer` is created, Cert Manager is ready to issue Let's Encrypt certificates for your ingresses. It takes about 5 minutes to set it up.

      If you enabled Grafana Loki or Prometheus, then you have already a Kubernetes Ingress that utilizes the certificates:

      ```
      kubectl get ingress -n infrastructure
      NAME      HOSTS                       ADDRESS           PORTS     AGE
      grafana   grafana.test.laszlo.cloud   172.104.145.220   80, 443   13h
      ```

      It binds under the `grafana` subdomain of the host that you set for Nginx, and has the `cert-manager.io/cluster-issuer` annotation that connects the Ingress with the ClusterIssuer,
      indicating that Cert Manager should provision a certificate.

      ```
      kubectl get ingress grafana -n infrastructure

      apiVersion: extensions/v1beta1
      kind: Ingress
      metadata:
        annotations:
          cert-manager.io/cluster-issuer: letsencrypt
          helm.fluxcd.io/antecedent: infrastructure:helmrelease/grafana
          kubernetes.io/ingress.class: nginx
        name: grafana
        namespace: infrastructure
      spec:
        rules:
        - host: grafana.test.laszlo.cloud
          http:
            paths:
            - backend:
                serviceName: grafana
                servicePort: 80
              path: /
        tls:
        - hosts:
          - grafana.test.laszlo.cloud
          secretName: tls-grafana
      ```
    schema: |-
      {
        "$schema": "http://json-schema.org/draft-07/schema",
        "$id": "#certManager",
        "type": "object",
        "properties": {
          "enabled": {
            "$id": "#/properties/enabled",
            "type": "boolean",
            "title": "Enabled"
          }
        },
        "dependencies": {
          "enabled": {
            "oneOf": [
              {
                "properties": {
                  "enabled": {
                    "const": false
                  }
                }
              },
              {
                "properties": {
                  "enabled": {
                    "const": true
                  },
                  "email": {
                    "$id": "#/properties/email",
                    "type": "string",
                    "title": "Administrator email",
                    "description": "Let's Encrypt will email you on this email upon expiring certificates"
                  }
                },
                "required": [
                  "email"
                ]
              }
            ]
          }
        }
      }
    uiSchema: |-
      [
        {
          "schemaIDs": [
            "#certManager"
          ],
          "uiSchema": {},
          "metaData": {}
        }
      ]
  - name: Loki
    variable: loki
    category: logging
    logo: https://raw.githubusercontent.com/gimlet-io/gimlet-stack-reference/main/assets/loki.png
    description: ''
    onePager: |-
      ### What do you get with Loki?

      - All your logs are forwarded to a dashboard
      - Flexible querying. See applications log realtime, or search for errors across all your apps
      - Log based alerts

      ### How to verify the deployment?
      ```
      $ kubectl get pods --namespace infrastructure

      NAME                         READY   STATUS    RESTARTS   AGE
      grafana-5dc6466b8d-2xkwf     1/1     Running   0          47h
      loki-promtail-bvn87          1/1     Running   0          32m
      loki-0                       1/1     Running   0          32m
      ```

      ### Logging in to Grafana

      Forward the internal ClusterIP to your laptop with:
      ```
      kubectl port-forward svc/grafana --namespace infrastructure 8888:80
      ```

      and access the dashboard on [http://localhost:8888](http://localhost:8888).

      Grafana generates an `admin` user password, and puts it into a Kubernetes secret. Grab it with:

      ```
      kubectl get secret grafana --namespace infrastructure --template='{{ index .data "admin-password"}}' | base64 -d
      ```

      Make sure to not include the trailing new line character: `%`

      On [http://localhost:8888/explore](http://localhost:8888/explore) start exploring the application logs by selecting the Loki datasource.
    schema: |-
      {
        "$schema": "http://json-schema.org/draft-07/schema",
        "$id": "http://example.com/example.json",
        "type": "object",
        "title": "The root schema",
        "description": "The root schema comprises the entire JSON document.",
        "properties": {
          "enabled": {
            "$id": "#/properties/enabled",
            "type": "boolean",
            "title": "Enabled"
          },
          "retentionDays": {
            "$id": "#/properties/retentionDays",
            "type": "integer",
            "title": "Retentian (in days)",
            "default": 14,
            "minimum": 1,
            "maximum": 365
          },
          "persistence": {
            "$id": "#/properties/persistence",
            "type": "boolean",
            "title": "Persistence"
          },
          "volumeSize": {
            "$id": "#/properties/volumeSize",
            "type": "integer",
            "title": "Peristent Volume Size (in GB)",
            "default": 10,
            "minimum": 1,
            "maximum": 100
          }
        }
      }
    uiSchema: |-
      [
        {
          "schemaIDs": [
            "#/properties/enabled",
            "#/properties/retentionDays",
            "#/properties/persistence",
            "#/properties/volumeSize"
          ],
          "uiSchema": {
            "#/properties/retentionDays": {
              "ui:widget": "range"
            },
            "#/properties/volumeSize": {
              "ui:widget": "range"
            }
          },
          "metaData": {}
        }
      ]
  - name: Prometheus
    variable: prometheus
    category: metrics
    logo: https://raw.githubusercontent.com/gimlet-io/gimlet-stack-reference/main/assets/prometheus.svg
    description: ''
    onePager: |-
      ### What do you get with Prometheus?
      - Infrastructure metrics and dashboards
      - Integration to application metrics
      - Default alerts and dashboards

      ### How to verify the deployment?

      ```
      $ kubectl get pods,svc --namespace infrastructure | grep 'prometheus\|grafana'

      NAME                                                   READY   STATUS    RESTARTS   AGE
      grafana-6987c9c5cf-gsr6x                               1/1     Running   0          66s
      prometheus-kube-state-metrics-c65b87574-qw56j          1/1     Running   0          65s
      prometheus-node-exporter-z8hgt                         1/1     Running   0          65s
      prometheus-pushgateway-7dc7cd5748-zzcc9                1/1     Running   0          65s
      prometheus-alertmanager-78b946d89-8djl8                1/2     Running   0          65s
      prometheus-server-b65c9b875-j62n8                      1/2     Running   0          65s
      service/grafana                               ClusterIP      10.43.64.217    <none>        80/TCP
      service/prometheus-alertmanager               ClusterIP      10.43.154.189   <none>        80/TCP
      service/prometheus-kube-state-metrics         ClusterIP      10.43.66.232    <none>        8080/TCP
      service/prometheus-server                     ClusterIP      10.43.133.254   <none>        80/TCP
      service/prometheus-node-exporter              ClusterIP      None            <none>        9100/TCP
      service/prometheus-pushgateway                ClusterIP      10.43.77.103    <none>        9091/TCP
      ```

      ### Browsing metrics with Grafana
      You can access the dashboards by forwarding the internal Grafana ClusterIP to your laptop with:
      ```
      kubectl port-forward svc/grafana --namespace infrastructure 8888:80
      ```

      and access the dashboard on [http://localhost:8888](http://localhost:8888).

      Grafana generates an `admin` user password, and puts it into a Kubernetes secret. Grab it with:

      ```
      kubectl get secret grafana --namespace infrastructure --template='{{ index .data "admin-password"}}' | base64 -d
      ```

      Make sure to not include the trailing new line character: `%`

    schema: |-
      {
        "$schema": "http://json-schema.org/draft-07/schema",
        "$id": "http://example.com/example.json",
        "type": "object",
        "title": "The root schema",
        "description": "The root schema comprises the entire JSON document.",
        "properties": {
          "enabled": {
            "$id": "#/properties/enabled",
            "type": "boolean",
            "title": "Enabled"
          },
          "retentionDays": {
            "$id": "#/properties/retentionDays",
            "type": "integer",
            "title": "Retentian (in days)",
            "default": 14,
            "minimum": 1,
            "maximum": 365
          },
          "persistence": {
            "$id": "#/properties/persistence",
            "type": "boolean",
            "title": "Persistence"
          },
          "volumeSize": {
            "$id": "#/properties/volumeSize",
            "type": "integer",
            "title": "Peristent Volume Size (in GB)",
            "default": 10,
            "minimum": 1,
            "maximum": 100
          }
        }
      }
    uiSchema: |-
      [
        {
          "schemaIDs": [
            "#/properties/enabled",
            "#/properties/retentionDays",
            "#/properties/persistence",
            "#/properties/volumeSize"
          ],
          "uiSchema": {
            "#/properties/retentionDays": {
              "ui:widget": "range"
            },
            "#/properties/volumeSize": {
              "ui:widget": "range"
            }
          },
          "metaData": {}
        }
      ]
  - name: Sealed Secrets
    variable: sealedSecrets
    category: secrets
    logo: https://raw.githubusercontent.com/gimlet-io/gimlet-stack-reference/main/assets/sealedSecrets.svg
    description: ''
    onePager: |-
      ### What do you get with Sealed Secrets?
      - A secret manager that allows a simple secret workflow for gitops repositories.
      - Uses asymmetric cryptography to encrypt secrets, just like HTTPS.
      ### How to verify the deployment?

      ```
      $ kubectl get pods --namespace infrastructure

      NAME                              READY   STATUS    RESTARTS   AGE
      sealed-secrets-7d7cc48f7f-q64sm   1/1     Running   0          14m
      ```

      Once the pod is running, secrets will be unsealed inside the cluster and made available for your applications.

      ### After verification

      In order to encrypt secrets, run the following steps:

      #### Install the `kubeseal` utility

      You will use this utility to encrypt secrets for your applications. It uses asymmetric crypto to encrypt secrets that only the controller can decrypt.

      Mac:
      ```
      brew install kubeseal
      ```

      Linux:
      ```
      wget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.12.6/kubeseal-linux-amd64 -O kubeseal
      sudo install -m 755 kubeseal /usr/local/bin/kubeseal
      ```

      #### Important steps after installing Sealed Secrets

      Perform these steps if you install Sealed Secrets for the first time, and you don't have encrypted secrets in your gitops repo yet.

      - fetch the sealing key

      ```
      kubeseal --fetch-cert --controller-namespace=infrastructure > sealing-key.pub
      ```

      This key is used to encrypt your secrets. It is only used to encrypt the secrets, noone can decrypt the secrets with this key, so you can share it with anyone.

      It is recommended that you add this key as a "Sealing Public Key" to 1-Click Infra bellow, so all keys in the gitops repo will be encrypted.

      - backup the master key

      ```
      kubectl get secret -n infrastructure -l sealedsecrets.bitnami.com/sealed-secrets-key -o yaml > master.key
      ```

      Keep this key secret, as this is the only key that can decrypt the secrets.
      Use it only in case of a cluster restoration.

      **Steps after restoration**

      Perform these steps if you are restoring your cluster, and you have encrypted secrets in your gitops repo already.

      Locate your backed up master key and apply it on the cluster.

      ```
      kubectl apply -f master.key
      ```

      Then restart the Sealed Secrets to pick up the master key:

      ```
      kubectl -n infrastructure delete pod -l name=sealed-secrets-controller
      ```

      Now your cluster can decrypt all secrets from the gitops repo.

    schema: |-
      {
        "$schema": "http://json-schema.org/draft-07/schema",
        "$id": "http://example.com/example.json",
        "type": "object",
        "title": "The root schema",
        "description": "The root schema comprises the entire JSON document.",
        "properties": {
          "enabled": {
            "$id": "#/properties/enabled",
            "type": "boolean",
            "title": "Enabled"
          }
        }
      }
    uiSchema: |-
      [
        {
          "schemaIDs": [
            "#/properties/enabled"
          ],
          "uiSchema": {},
          "metaData": {}
        }
      ]
  - name: Grafana Tempo
    variable: tempo
    category: tracing
    logo: https://raw.githubusercontent.com/gimlet-io/gimlet-stack-reference/main/assets/tempo.png
    description: ''
    onePager: |-
      ### What do you get with Tempo?
      - Grafana Tempo is an open source, easy-to-use and high-volume distributed tracing backend. Tempo is cost-efficient, requiring only object storage to operate, and is deeply integrated with Grafana, Prometheus, and Loki.

      ### How to verify the deployment?

      ```
      $ kubectl get pods --namespace infrastructure | grep tempo

      NAME                              READY   STATUS    RESTARTS   AGE
      tempo-0  1/1     Running   0          1m
      ```


    schema: |-
      {
        "$schema": "http://json-schema.org/draft-07/schema",
        "$id": "http://example.com/example.json",
        "type": "object",
        "title": "The root schema",
        "description": "The root schema comprises the entire JSON document.",
        "properties": {
          "enabled": {
            "$id": "#/properties/enabled",
            "type": "boolean",
            "title": "Enabled"
          },
           "sampleTraceData": {
            "$id": "#/properties/sampleTraceData",
            "type": "boolean",
            "title": "Sample Trace Data"
          }
        }
      }
    uiSchema: |-
      [
        {
          "schemaIDs": [
            "#/properties/enabled",
            "#/properties/sampleTraceData"
          ],
          "uiSchema": {},
          "metaData": {}
        }
      ]
  - name: Kyverno
    variable: kyverno
    category: policy
    logo: https://raw.githubusercontent.com/kyverno/kyverno/main/img/logo.png
    description: ''
    onePager: |-
      ### What do you get with Kyverno?
      - Kyverno is a policy engine designed for Kubernetes. It can validate, mutate, and generate configurations using admission controls and background scans. Kyverno policies are Kubernetes resources and do not require learning a new language. Kyverno is designed to work nicely with tools you already use like kubectl, kustomize, and Git.

      ### How to verify the deployment?

      ```
      $ kubectl get pods --namespace infrastructure | grep kyverno

      NAME                              READY   STATUS    RESTARTS   AGE
      kyverno-5ccd8d64b4-5vqlk  1/1     Running   0          1m
      ```


    schema: |-
      {
        "$schema": "http://json-schema.org/draft-07/schema",
        "$id": "http://example.com/example.json",
        "type": "object",
        "title": "The root schema",
        "description": "The root schema comprises the entire JSON document.",
        "properties": {
          "enabled": {
            "$id": "#/properties/enabled",
            "type": "boolean",
            "title": "Enabled"
          },
          "podSecurityStandard": {
            "$id": "#/properties/podSecurityStandard",
            "type": "string",
            "title": "Security Standard",
            "enum": [
              "baseline",
              "restricted"
            ],
            "default": "baseline"
          }
        }
      }
    uiSchema: |-
      [
        {
          "schemaIDs": [
            "#/properties/enabled",
            "#/properties/podSecurityStandard"
          ],
          "uiSchema": {},
          "metaData": {}
        }
      ]

changeLog: |

  - üÜï Kyverno Added
    ‚ùó Default policy validation is audit.

message: |

  Hey üëã Laszlo here, the maker of Gimlet Stack.

  Thank you for using Gimlet Stack!

  My goal is to help you

  - to get started with your Kubernetes stack
  - to navigate new versions
  - and to give you best practices

  Now that you generated Kubernetes resources from your stack, maybe you are interested in:

  - how to upgrade your stack üëâ https://gimlet.io/gimlet-stack/upgrading-a-stack/
  - or how to reuse the automation to deliver your own apps üëâ https://gimlet.io/gimlet-stack/reusing-the-stack-repo-for-application-deployments/

  I also recommend that you sign up for the stack update feed.
  You will get 2 emails a month helping you keeping your cluster up-to date and secure:
  üëâ https://stack-update-feed.gimlet.io/

  Onwards!
